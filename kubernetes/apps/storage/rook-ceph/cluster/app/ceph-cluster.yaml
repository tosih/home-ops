---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: storage
spec:
  dataDirHostPath: /var/lib/rook
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: false
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: rook
        enabled: true
  dashboard:
    enabled: true
    ssl: false
  crashCollector:
    disable: false
  storage:
    useAllNodes: false
    useAllDevices: false
    # Using directory storage on EPHEMERAL partition
    # Talos workers have single 256GB NVMe disks with EPHEMERAL consuming all available space
    # Raw block devices not viable without dedicated storage disks
    config:
      osdsPerDevice: "1"
    nodes:
      - name: worker0
        config:
          directories:
            - path: /var/lib/rook/osd0
      - name: worker1
        config:
          directories:
            - path: /var/lib/rook/osd0
      - name: worker2
        config:
          directories:
            - path: /var/lib/rook/osd0
  resources:
    mon:
      requests:
        cpu: "250m"
        memory: "512Mi"
      limits:
        memory: "1Gi"
    osd:
      requests:
        cpu: "500m"
        memory: "1Gi"
      limits:
        memory: "2Gi"
    mgr:
      requests:
        cpu: "250m"
        memory: "256Mi"
      limits:
        memory: "512Mi"
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
  healthCheck:
    daemonHealth:
      mon:
        interval: 45s
        timeout: 600s
      osd:
        interval: 60s
        timeout: 600s
      status:
        interval: 60s
