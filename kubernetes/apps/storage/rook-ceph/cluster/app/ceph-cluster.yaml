---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: storage
spec:
  dataDirHostPath: /var/lib/rook
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: false
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: rook
        enabled: true
  dashboard:
    enabled: true
    ssl: false
  crashCollector:
    disable: false
  storage:
    useAllNodes: true
    useAllDevices: false
    # Using PVC-based storage on EPHEMERAL partition via local-hostpath StorageClass
    # Talos workers have single 256GB NVMe disks with EPHEMERAL consuming all available space
    # Raw block devices not viable without dedicated storage disks
    storageClassDeviceSets:
      - name: data
        count: 3  # 3 OSDs total (1 per worker)
        portable: false
        encrypted: false
        volumeClaimTemplates:
          - metadata:
              name: data
            spec:
              resources:
                requests:
                  storage: 80Gi  # Allocate 80GB per OSD from hostPath
              storageClassName: local-hostpath
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
        placement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: node-role.kubernetes.io/worker
                      operator: Exists
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd
                  topologyKey: kubernetes.io/hostname
  resources:
    mon:
      requests:
        cpu: "250m"
        memory: "512Mi"
      limits:
        memory: "1Gi"
    osd:
      requests:
        cpu: "500m"
        memory: "1Gi"
      limits:
        memory: "2Gi"
    mgr:
      requests:
        cpu: "250m"
        memory: "256Mi"
      limits:
        memory: "512Mi"
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
  healthCheck:
    daemonHealth:
      mon:
        interval: 45s
        timeout: 600s
      osd:
        interval: 60s
        timeout: 600s
      status:
        interval: 60s
